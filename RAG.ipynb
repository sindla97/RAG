{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO/nPjUvEly4zHVJdYJcUha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sindla97/RAG/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aproach\n",
        "1. import pdf files to the knowledge base\n",
        "2. Perfrom semantic segmentation using a sentense transfromer type model and convert them to embeddings\n",
        "3. use a rerank model to rank the retrived embeddings\n",
        "4. Use the LLM to generate response based on the query and retrvied emebeddings\n",
        "\n",
        "\n",
        "Try to provide an option to update the knowlege base when new documents are provided,"
      ],
      "metadata": {
        "id": "BMD_8mwCcquD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sMho2se9bUp9",
        "outputId": "a3df6bfe-b50a-4b1b-8f52-97f937622060",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting langchain-pinecone\n",
            "  Downloading langchain_pinecone-0.2.8-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.68 (from langchain-google-genai)\n",
            "  Downloading langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain_experimental)\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pinecone<8.0.0,>=6.0.0 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (2.0.2)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain-pinecone)\n",
            "  Downloading langchain_tests-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-openai>=0.3.11 (from langchain-pinecone)\n",
            "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (6.31.1)\n",
            "Collecting langchain<1.0.0,>=0.3.26 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting langsmith>=0.1.125 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading langsmith-0.4.4-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Collecting openai<2.0.0,>=1.86.0 (from langchain-openai>=0.3.11->langchain-pinecone)\n",
            "  Downloading openai-1.93.3-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai>=0.3.11->langchain-pinecone)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.3.5)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.28.1)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting vcrpy>=7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2025.6.15)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.4.0)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai)\n",
            "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.16.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading orjson-3.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain-pinecone) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain-pinecone)\n",
            "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain-pinecone) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain-pinecone) (4.67.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.2)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain-pinecone) (2024.11.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from vcrpy>=7.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.17.2)\n",
            "Collecting py-cpuinfo (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.17.1)\n",
            "Requirement already satisfied: rich>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (14.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.19.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.1.2)\n",
            "Downloading langchain_google_genai-2.1.7-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_pinecone-0.2.8-py3-none-any.whl (22 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tests-0.3.20-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.4.4-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.93.3-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.1/755.1 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (25 kB)\n",
            "Downloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
            "Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (585 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.5/585.5 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_status-1.73.1-py3-none-any.whl (14 kB)\n",
            "Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: py-cpuinfo, filetype, zstandard, python-dotenv, pypdf, pinecone-plugin-interface, packaging, orjson, mypy-extensions, jsonpointer, jiter, httpx-sse, greenlet, vcrpy, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, pinecone-plugin-assistant, marshmallow, jsonpatch, grpcio-status, syrupy, pytest-socket, pytest-recording, pytest-codspeed, pytest-benchmark, pytest-asyncio, pydantic-settings, pinecone, openai, langsmith, dataclasses-json, aiohttp-retry, langchain-core, langchain-text-splitters, langchain-tests, langchain-openai, google-ai-generativelanguage, langchain-pinecone, langchain-google-genai, langchain, langchain-community, langchain_experimental\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "Successfully installed SQLAlchemy-2.0.41 aiohttp-retry-2.9.1 dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.6.18 greenlet-3.2.3 grpcio-status-1.73.1 httpx-sse-0.4.1 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.26 langchain-community-0.3.27 langchain-core-0.3.68 langchain-google-genai-2.1.7 langchain-openai-0.3.27 langchain-pinecone-0.2.8 langchain-tests-0.3.20 langchain-text-splitters-0.3.8 langchain_experimental-0.3.4 langsmith-0.4.4 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-1.93.3 orjson-3.10.18 packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7 py-cpuinfo-9.0.0 pydantic-settings-2.10.1 pypdf-5.7.0 pytest-asyncio-0.26.0 pytest-benchmark-5.1.0 pytest-codspeed-3.2.0 pytest-recording-0.13.4 pytest-socket-0.7.0 python-dotenv-1.1.1 requests-toolbelt-1.0.0 syrupy-4.9.1 tiktoken-0.9.0 typing-inspect-0.9.0 vcrpy-7.0.0 zstandard-0.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "packaging"
                ]
              },
              "id": "752d32c9db674bbb99029e3ca3a15a75"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain-google-genai pypdf langchain_experimental langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_path = \"/content/AI_Agent_white_paper_by_google_1737132048.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)"
      ],
      "metadata": {
        "id": "f7qhdF00i8_5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pages=loader.load()\n",
        "# we can use load_and_split() but we have to provide the textsplitter criterion if not it uses Recrusivetextsplitter by default\n",
        "all_pages[10].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "b8qIXqDQ1CYi",
        "outputId": "75ce790d-0721-4cfb-ec51-c0c866d1af08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agents\\n11\\nSeptember 2024\\nd. Action input: The model’s decision on what inputs to provide to the tool (if any)\\ne. Observation: The result of the action / action input sequence\\ni. This thought / action / action input / observation could repeat N-times as needed\\nf. Final answer: The model’s final answer to provide to the original user query\\n4. The ReAct loop concludes and a final answer is provided back to the user\\nFigure 2. Example agent with ReAct reasoning in the orchestration layer\\nAs shown in Figure 2, the model, tools, and agent configuration work together to provide \\na grounded, concise response back to the user based on the user’s original query. While \\nthe model could have guessed at an answer (hallucinated) based on its prior knowledge, \\nit instead used a tool (Flights) to search for real-time external information. This additional \\ninformation was provided to the model, allowing it to make a more informed decision based \\non real factual data and to summarize this information back to the user.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "google_api_key=userdata.get('GOOGLE_API_KEY')\n"
      ],
      "metadata": {
        "id": "I6rV34EQm8yq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = SemanticChunker(GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key))\n",
        "chunks = text_splitter.create_documents([''.join([page.page_content for page in all_pages])]) # create document takes in list of dcoumetns and splits it, we have to pass the whole document as string"
      ],
      "metadata": {
        "id": "ROzrTdWJ0u76"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "b0RqkIoB3REl",
        "outputId": "7ec12a78-8ed5-4f8f-d23c-ca747ed031fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agents\\nAuthors: Julia Wiesinger, Patrick Marlow  \\nand Vladimir VuskovicAgents\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael LanningIntroduction 4\\nWhat is an agent? 5\\n The model 6\\n The tools 7\\n The orchestration layer 7\\n Agents vs. models 8\\n Cognitive architectures: How agents operate  8\\nTools: Our keys to the outside world 12\\n Extensions  13\\n  Sample Extensions  15\\n Functions  18\\n  Use cases 21\\n  Function sample code 24\\n Data stores 27\\n  Implementation and application 28\\n Tools recap 32\\nEnhancing model performance with targeted learning 33\\nAgent quick start with LangChain 35\\nProduction applications with Vertex AI agents 38\\nSummary 40\\nEndnotes 42\\nTable of contentsAgents\\n4\\nSeptember 2024\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MziuiygDp7L4",
        "outputId": "bf3f37c5-59e9-4243-8e02-5eae55c2b32e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key).embed_documents([ x.page_content for x in chunks])\n",
        "# there are two options to generate embedding on is using embed_documents( which takes in list of strings) & embed_query(takes a string and converts to embeddings)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Zepz3uSttt3Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets use a vector store  to store these embeddings\n",
        "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
        "vecstore=PineconeVectorStore(embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key),pinecone_api_key=userdata.get('pinecone_api'), index_name='rag-test')\n",
        "vecstore"
      ],
      "metadata": {
        "id": "kgW4RNIC4q_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ee7e1049-edab-4a56-dd2f-5bceca108991"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7a22c3fe1310>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vecstore.add_documents(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZqbvWGIqlSA",
        "outputId": "e71a62b3-1fcb-4687-dada-8546b31017cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dd069f50-3ea4-43b2-a167-043e83cf958f',\n",
              " '59dfa83a-723d-43a1-bf8a-e81bb015b475',\n",
              " 'dee5fe7e-02f1-43a0-8d8e-5d18598e434d',\n",
              " 'd8426ee9-29cc-4235-ae64-56bdc96cafbb',\n",
              " 'f2060c27-0949-4721-a294-914dcec8526a',\n",
              " 'cdb17c59-9486-41f0-8ba0-427381acaea5',\n",
              " '206825cc-4743-417f-aa09-42cab29dd84d',\n",
              " '55636f5b-5596-4e3a-8add-9dcaa1d289a5',\n",
              " '14f6e346-987e-4dac-b21b-65b797c3e0c7',\n",
              " '6f274d2e-d054-4a4a-b17d-a10cd934b6e1',\n",
              " '10046cdb-15f6-4869-91bb-e5221c2df26a',\n",
              " '57cfe15b-1227-42f7-bc6f-ad81885ffce0',\n",
              " 'f5b75702-52ba-4c42-9c8d-09dcd04a8c97',\n",
              " '5503f731-1682-4156-bcec-37249a939281',\n",
              " '671577da-39bf-4be9-a47a-7029d6adb356',\n",
              " 'a4c8be95-5fa6-4593-abfa-4da42159b1e5',\n",
              " '8075956f-5be1-413f-9030-742f127c1871',\n",
              " '1e3cee1f-c08b-40de-b0a8-153cbf0029e4']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vecstore.search(query='what are the contents of the document Agentic AI', search_type='similarity')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuklbpxE430i",
        "outputId": "0ec955ef-ec44-4540-8820-201457571bb6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='8075956f-5be1-413f-9030-742f127c1871', metadata={}, page_content=\"6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  \\nhttps://arxiv.org/abs/2305.10601 . 7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  \\nhttps://arxiv.org/abs/2305.08291 .\"),\n",
              " Document(id='5503f731-1682-4156-bcec-37249a939281', metadata={}, page_content=\"Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  \\nhttps://arxiv.org/pdf/2302.12246.pdf .\"),\n",
              " Document(id='dd069f50-3ea4-43b2-a167-043e83cf958f', metadata={}, page_content='Agents\\nAuthors: Julia Wiesinger, Patrick Marlow  \\nand Vladimir VuskovicAgents\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael LanningIntroduction 4\\nWhat is an agent? 5\\n The model 6\\n The tools 7\\n The orchestration layer 7\\n Agents vs. models 8\\n Cognitive architectures: How agents operate  8\\nTools: Our keys to the outside world 12\\n Extensions  13\\n  Sample Extensions  15\\n Functions  18\\n  Use cases 21\\n  Function sample code 24\\n Data stores 27\\n  Implementation and application 28\\n Tools recap 32\\nEnhancing model performance with targeted learning 33\\nAgent quick start with LangChain 35\\nProduction applications with Vertex AI agents 38\\nSummary 40\\nEndnotes 42\\nTable of contentsAgents\\n4\\nSeptember 2024\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion.'),\n",
              " Document(id='a4c8be95-5fa6-4593-abfa-4da42159b1e5', metadata={}, page_content=\"Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'. Available at:  \\nhttps://arxiv.org/abs/2302.00923 .\")]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vecstore.search(query='what is the summary of the document Agentic AI', search_type='similarity', k=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaKul1SK8rVT",
        "outputId": "34134a00-61c5-45da-fbcb-602cfe763ec1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='8075956f-5be1-413f-9030-742f127c1871', metadata={}, page_content=\"6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  \\nhttps://arxiv.org/abs/2305.10601 . 7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  \\nhttps://arxiv.org/abs/2305.08291 .\"),\n",
              " Document(id='10046cdb-15f6-4869-91bb-e5221c2df26a', metadata={}, page_content='3.'),\n",
              " Document(id='5503f731-1682-4156-bcec-37249a939281', metadata={}, page_content=\"Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  \\nhttps://arxiv.org/pdf/2302.12246.pdf .\"),\n",
              " Document(id='f5b75702-52ba-4c42-9c8d-09dcd04a8c97', metadata={}, page_content='4.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# send the retrived documents to llm and ouptut in a structured format\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "output_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", google_api_key=google_api_key)\n",
        "#llm.invoke(\"Write me a ballad about LangChain\")\n",
        "#lets build a message\n"
      ],
      "metadata": {
        "id": "hjA733Tl9Drl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query='what is the summary of the document Agentic AI'\n",
        "retrived_contents=vecstore.search(query=user_query, search_type='similarity', k=4)\n",
        "message=f\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\n",
        "output_llm.invoke(message).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "6QOEGJcbDimx",
        "outputId": "b38f1c4b-f5ab-4bee-87e4-f42b41c46750"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided document snippets, there is no mention of \"Agentic AI\". Therefore, I cannot provide a summary of it.\\n\\nHowever, I can tell you that the documents discuss techniques for improving the problem-solving capabilities of Large Language Models (LLMs). These techniques include:\\n\\n*   **Tree of Thoughts (ToT):** A method that allows LLMs to explore multiple reasoning paths and evaluate different options to solve complex problems.\\n*   **Large Language Model Guided Tree-of-Thought:** Likely a variation or application of the ToT approach, possibly with specific guidance mechanisms.\\n*   **Active Prompting with Chain-of-Thought:** A technique that uses targeted prompts to guide LLMs through a reasoning process, encouraging them to break down problems into smaller, more manageable steps.\\n\\nSince I can\\'t summarize \"Agentic AI\" based on the provided context, I will ask a follow-up question related to the techniques that *are* mentioned:\\n\\n**Follow-up question:** How do Tree of Thoughts and Active Prompting with Chain-of-Thought differ in their approach to guiding LLMs through problem-solving, and what are the potential advantages and disadvantages of each method?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(user_query):\n",
        "  retrived_contents=vecstore.search(query=user_query, search_type='similarity', k=4)\n",
        "  message=f\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\n",
        "  print(output_llm.invoke(message).content)\n",
        "  new_user_query=input()\n",
        "  if len(new_user_query)>0:\n",
        "    chat(new_user_query)\n",
        "\n",
        "chat('what are the contents of the document Agentic AI')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rw1DA-GTEydJ",
        "outputId": "c43cb60b-6428-42bd-df71-9e0b8cc0a13c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, based on the provided text, here's a structured summary of the document \"Agents\" (dated September 2024):\n",
            "\n",
            "**Document Title:** Agents\n",
            "\n",
            "**Authors:** Julia Wiesinger, Patrick Marlow, Vladimir Vuskovic\n",
            "\n",
            "**Acknowledgements (Reviewers and Contributors):** Evan Huang, Emily Xue, Olcan Sercinoglu, Sebastian Riedel, Satinder Baveja, Antonio Gulli, Anant Nawalgaria\n",
            "\n",
            "**Curators and Editors:** Antonio Gulli, Anant Nawalgaria, Grace Mollison\n",
            "\n",
            "**Technical Writer:** Joey Haymaker\n",
            "\n",
            "**Designer:** Michael Lanning\n",
            "\n",
            "**Table of Contents/Sections:**\n",
            "\n",
            "*   Introduction\n",
            "*   What is an agent?\n",
            "    *   The model\n",
            "    *   The tools\n",
            "    *   The orchestration layer\n",
            "    *   Agents vs. models\n",
            "    *   Cognitive architectures: How agents operate\n",
            "*   Tools: Our keys to the outside world\n",
            "    *   Extensions\n",
            "        *   Sample Extensions\n",
            "    *   Functions\n",
            "        *   Use cases\n",
            "        *   Function sample code\n",
            "    *   Data stores\n",
            "        *   Implementation and application\n",
            "    *   Tools recap\n",
            "*   Enhancing model performance with targeted learning\n",
            "*   Agent quick start with LangChain\n",
            "*   Production applications with Vertex AI agents\n",
            "*   Summary\n",
            "*   Endnotes\n",
            "\n",
            "**Key Concepts/Themes (based on section titles):**\n",
            "\n",
            "*   **Agent Definition and Components:** The document will define what an agent is and likely discuss the roles of models, tools, and an orchestration layer in agent architecture.\n",
            "*   **Cognitive Architectures:**  It will explore how agents operate, potentially covering different architectural approaches.\n",
            "*   **Tools:** A significant portion of the document is dedicated to tools that agents use to interact with the outside world, including extensions, functions, and data stores.\n",
            "*   **Agent Development and Deployment:** The document includes practical aspects like using LangChain for agent development and deploying agents with Vertex AI.\n",
            "*   **Model Performance Enhancement:**  The document touches on how to improve model performance through targeted learning.\n",
            "\n",
            "**External Resources Referenced:**\n",
            "\n",
            "*   Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'. Available at: https://arxiv.org/abs/2302.00923\n",
            "*   Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at: https://arxiv.org/abs/2305.10601\n",
            "*   Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at: https://arxiv.org/abs/2305.08291\n",
            "*   Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at: https://arxiv.org/pdf/2302.12246.pdf\n",
            "\n",
            "**Follow-up Question:**\n",
            "\n",
            "Given the focus on \"Tools\" and the mention of \"Extensions,\" \"Functions,\" and \"Data stores,\" can you elaborate on the specific *types* of extensions, functions, and data stores that are discussed in the document? For example, what are some specific examples of extensions mentioned?\n",
            "why do alms are used in agents AI\n",
            "Based on the provided references:\n",
            "\n",
            "Large Language Models (LLMs) are used in AI agents, specifically in the context of \"Tree of Thoughts\" (ToT) approaches, to enable deliberate problem-solving.\n",
            "\n",
            "*   **Tree of Thoughts:** The references highlight the use of LLMs in a structured approach called \"Tree of Thoughts.\" This method aims to improve problem-solving capabilities by allowing the AI agent to explore multiple reasoning paths (\"thoughts\") organized in a tree-like structure. The LLM is used to generate, evaluate, and select these thoughts, enabling more systematic and effective problem-solving compared to simple chain-of-thought prompting.\n",
            "\n",
            "**Follow-up question:**\n",
            "\n",
            "How do the \"Tree of Thoughts\" approaches described in these papers address the limitations of traditional chain-of-thought prompting in complex reasoning tasks?\n",
            "can you answer your followup question\n",
            "Okay, based on the provided citations, I can answer my followup question in a structured format and ask another followup question.\n",
            "\n",
            "**Structured Answer:**\n",
            "\n",
            "Based on the citations, \"Tree of Thoughts\" (ToT) is a problem-solving approach that utilizes Large Language Models (LLMs) to explore multiple reasoning paths. The key concepts are:\n",
            "\n",
            "*   **Citation:** Yao et al., 2023 (Citation 6)\n",
            "*   **Core Idea:** ToT enables LLMs to explore different thought paths, branching out and evaluating each path to find the optimal solution.  This contrasts with linear, chain-of-thought reasoning.\n",
            "*   **Citation:** Long, 2023 (Citation 7)\n",
            "*   **Core Idea:** ToT uses LLMs to guide the generation and evaluation of these thought branches.\n",
            "\n",
            "**Follow-up Question:**\n",
            "\n",
            "Given that both papers (Yao et al. and Long) focus on Tree of Thoughts, what are the key differences in their proposed implementations or applications of the ToT framework?\n",
            "what is the difference between agents and models\n",
            "Based on the provided entries (3.4.6 and 7), the difference between agents and models in the context of \"Tree of Thoughts\" can be structured as follows:\n",
            "\n",
            "*   **Models:** These are the underlying Large Language Models (LLMs) themselves. They are the foundation upon which problem-solving is built. The papers mention the use of LLMs but don't explicitly define them *as* agents. The models are responsible for generating thoughts, evaluating them, and performing actions based on prompts.\n",
            "*   **Agents:** The \"Tree of Thoughts\" framework uses the LLM as the *core* but builds an *agent* around it. The agent encompasses the LLM *and* the strategic reasoning process. The \"Tree of Thoughts\" approach empowers the LLM with a deliberate problem-solving strategy that goes beyond simple prompting.\n",
            "\n",
            "In essence, the LLM *is* the model, while the agent *is* the model *plus* a specific reasoning and search strategy (e.g., Tree of Thoughts) that guides its use.\n",
            "\n",
            "**Follow-up Question:**\n",
            "\n",
            "How does the choice of a specific search algorithm (e.g., breadth-first search, depth-first search, Monte Carlo Tree Search) within the Tree of Thoughts framework affect the overall performance and resource consumption of the agent?\n",
            "can you briefly explain how agents operate\n",
            "Okay, based on the provided text, here's a brief explanation of how agents operate, focusing on the client vs. agent-side control aspect, in a structured format:\n",
            "\n",
            "**How Agents Operate (with Client/Agent Control Focus):**\n",
            "\n",
            "*   **Core Idea:** Agents (like a travel concierge) use a language model to interact with users and perform tasks (like booking trips).\n",
            "*   **Function Calling/Extensions:** Agents can use \"functions\" or \"extensions\" to perform these tasks.\n",
            "*   **Client-Side vs. Agent-Side Control:**\n",
            "    *   **Agent-Side Control (Extensions - Implied):** The language model manages the entire API execution process. This means the model decides *how* and *when* to call the function/extension.\n",
            "    *   **Client-Side Control (Function Calling - Implied):** The agent *developer* wants more control. The language model *identifies the need* to use a function, but the actual execution and management of the API call is handled by the client (the developer's code). This is useful when the developer doesn't want the language model to manage the API.\n",
            "*   **Use Case Example:** A travel concierge agent uses functions to book flights, hotels, etc. The developer might want client-side control for sensitive operations or complex execution flows.\n",
            "\n",
            "**Follow-up Question:**\n",
            "\n",
            "Given that client-side control allows developers to manage API execution directly, what are some specific scenarios where a developer would *prefer* this approach over having the language model manage the API calls directly?\n",
            "what type of tools are used and what is a tool \n",
            "Okay, based on the provided references, here's a structured answer regarding the tools used:\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "*   **Type of Tools Used:** Large Language Models (LLMs)\n",
            "*   **What is a Tool (in this context):** In the context of these papers, an LLM serves as the central \"tool.\" It is used for:\n",
            "    *   **Problem Solving:** LLMs are employed to tackle complex problems through a deliberate and structured approach.\n",
            "    *   **Thought Generation:** The LLM is used to generate multiple \"thoughts\" or potential solutions/approaches.\n",
            "    *   **Thought Evaluation:** The LLM is used to evaluate the generated thoughts and determine their viability.\n",
            "    *   **Tree-of-Thought Implementation:** The LLM is the core component that enables the implementation of the \"Tree of Thoughts\" methodology, allowing for exploration of different solution paths.\n",
            "    *   **Guidance:** LLMs guide the exploration and evaluation within the Tree-of-Thought framework.\n",
            "\n",
            "**Follow-up Question:**\n",
            "\n",
            "Given that both papers focus on \"Tree of Thoughts\" using LLMs, what are the key differences in their approaches to implementing or guiding the Tree of Thoughts process, and how might these differences affect the performance or applicability of each method?\n",
            "how are api's used for building agents ai models\n",
            "Here's a structured answer on how APIs are used for building AI agents, based on the provided content, along with a follow-up question:\n",
            "\n",
            "**How APIs are Used for Building AI Agents**\n",
            "\n",
            "The document describes the use of **tools**, including Extensions and Functions, as keys to the outside world for AI agents. APIs are fundamental to how these tools operate, enabling agents to interact with external services and data.\n",
            "\n",
            "*   **Handling Complex Client-Side Execution Flows:** APIs are used to handle complex, client-side execution flows for the end user. This is particularly useful where the agent developer does not want the language model to manage the API execution directly (as with Extensions).\n",
            "*   **Extensions:** Extensions are tools that allow agents to interact with external services. APIs are the means by which these extensions communicate with and utilize those services.\n",
            "*   **Functions:** Functions are also tools that an agent can use. APIs are the mechanism through which the agent invokes and uses these functions. The document provides use cases where a model can invoke functions to handle complex client-side execution flows.\n",
            "*   **Travel Concierge Example:** An example of a travel concierge agent is given, where the agent interacts with users to book vacation trips. APIs would be used to interact with travel booking services (e.g., to search for flights, hotels, etc.).\n",
            "*   **Data Stores:** APIs are used to access and interact with data stores, allowing agents to retrieve and store information.\n",
            "*   **Tools Recap:** The section \"Tools Recap\" reinforces the importance of tools (powered by APIs) in enabling agents to perform tasks.\n",
            "\n",
            "**Follow-up Question:**\n",
            "\n",
            "How does the \"orchestration layer\" (mentioned on page 7) manage the interaction between the language model, the tools (APIs), and the user within the agent architecture?\n",
            "who is kavya\n",
            "Based on the provided text, there is no mention of anyone named \"Kavya.\" The provided text only contains references to two papers:\n",
            "\n",
            "*   **Yao, S. et al. (2023):** \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\"\n",
            "*   **Long, X. (2023):** \"Large Language Model Guided Tree-of-Thought\"\n",
            "\n",
            "**Answer:** Kavya is not mentioned in the provided text.\n",
            "\n",
            "**Follow-up Question:** Is there any other context or information available that might help identify who Kavya is, or what her relevance might be to the topic of large language models and tree-of-thought approaches?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what are the contents of the document Agentic AI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnew_user_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what are the contents of the document Agentic AI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnew_user_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what are the contents of the document Agentic AI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnew_user_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what are the contents of the document Agentic AI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnew_user_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what are the contents of the document Agentic AI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnew_user_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what are the contents of the document Agentic AI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnew_user_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what are the contents of the document Agentic AI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnew_user_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what are the contents of the document Agentic AI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-575089422.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mnew_user_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}