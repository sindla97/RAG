{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sindla97/RAG/blob/dev/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMD_8mwCcquD"
      },
      "source": [
        "# Aproach\n",
        "1. import pdf files to the knowledge base\n",
        "2. Perfrom semantic segmentation using a sentense transfromer type model and convert them to embeddings\n",
        "3. use a rerank model to rank the retrived embeddings\n",
        "4. Use the LLM to generate response based on the query and retrvied emebeddings\n",
        "\n",
        "\n",
        "Try to provide an option to update the knowlege base when new documents are provided,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sMho2se9bUp9"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-google-genai pypdf langchain_experimental langchain-pinecone langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7qhdF00i8_5"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_path = \"/content/AI_Agent_white_paper_by_google_1737132048.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "loader2=PyPDFLoader(pdf_path, mode='single')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8qIXqDQ1CYi"
      },
      "outputs": [],
      "source": [
        "all_pages=loader.load()\n",
        "# we can use load_and_split() but we have to provide the textsplitter criterion if not it uses Recrusivetextsplitter by default\n",
        "all_pages[10].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading the whole document and invoking the llm for summary and generating embeddings would work for small documents but when the documnet size is large, it has to be chunked and summarized indiviudally and combine to generating embeddings\n"
      ],
      "metadata": {
        "id": "JyJqtQ8ESGaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarize using the whole document"
      ],
      "metadata": {
        "id": "nc8SdLqfTYUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whole_doc=loader2.load() # loading the whole pdf as a single document for answering summary type questions\n",
        "whole_doc[0].page_content"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jcSkYs-lDdYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6rV34EQm8yq"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "google_api_key=userdata.get('GOOGLE_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROzrTdWJ0u76"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = SemanticChunker(GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key))\n",
        "chunks = text_splitter.create_documents([''.join([page.page_content for page in all_pages])]) # create document takes in list of dcoumetns and splits it, we have to pass the whole document as string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0RqkIoB3REl"
      },
      "outputs": [],
      "source": [
        "chunks[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "i8I_4jTXEfSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjA733Tl9Drl"
      },
      "outputs": [],
      "source": [
        "# importing the llm to summarize the whole document  an retrvial\n",
        "#send the retrived documents to llm and ouptut in a structured format\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "output_llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=google_api_key)\n",
        "\n",
        "#lets build a message\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"please provide a summary of this document\"),\n",
        "    (\"human\", f\"please proivde a detailed summary of the document: {whole_doc[0].page_content}\"),\n",
        "]\n",
        "summary=output_llm.invoke(messages)\n",
        "summary # summary of the whole document"
      ],
      "metadata": {
        "id": "p5bhGWs1FE7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "ai_message = summary.content\n",
        "doc = Document(page_content=ai_message) # convert the AI message output to a documemnt\n",
        "\n",
        "print(doc)"
      ],
      "metadata": {
        "id": "6R5Dg20fGwBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MziuiygDp7L4"
      },
      "outputs": [],
      "source": [
        "chunks.append(doc) # append the document tothe other chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kgW4RNIC4q_x"
      },
      "outputs": [],
      "source": [
        "# lets use a vector store  to store these embeddings\n",
        "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
        "vecstore=PineconeVectorStore(embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key),pinecone_api_key=userdata.get('pinecone_api'), index_name='rag-test')\n",
        "vecstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZqbvWGIqlSA"
      },
      "outputs": [],
      "source": [
        "vecstore.delete(delete_all=True) # delete any existing embeddings in the index\n",
        "vecstore.add_documents(chunks) # create embeddings (based on chunk's text) and add to Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuklbpxE430i"
      },
      "outputs": [],
      "source": [
        "vecstore.search(query='what are the contents of the document Agentic AI', search_type='similarity') # quick check for the retrival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaKul1SK8rVT"
      },
      "outputs": [],
      "source": [
        "vecstore.search(query='what is the summary of the document Agentic AI', search_type='similarity', k=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QOEGJcbDimx"
      },
      "outputs": [],
      "source": [
        "user_query='what is the summary of the document Agents'\n",
        "retrived_contents=vecstore.search(query=user_query, search_type='similarity', k=4)\n",
        "message=f\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\n",
        "print(output_llm.invoke(message).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Rw1DA-GTEydJ"
      },
      "outputs": [],
      "source": [
        "def chat(user_query):\n",
        "  retrived_contents=vecstore.search(query=user_query, search_type='similarity', k=4)\n",
        "  message=f\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\n",
        "  print(output_llm.invoke(message).content)\n",
        "  new_user_query=input()\n",
        "  if len(new_user_query)>0:\n",
        "    chat(new_user_query)\n",
        "\n",
        "chat('what are the contents of the document Agentic AI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB769BVr1JJv"
      },
      "source": [
        "# lets summarize the whole document and add it to the vector store\n",
        "we can utilize langchain summarize feature but it requires frequent invokes with the llm, if the llm is hosted and accessed through inference endpoints it does cost and trigger alerts for quota and usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWwloSJ4n3QE"
      },
      "source": [
        "## To access models in huggingface we have2 options\n",
        "### 1. using HuggingFaceEndpoint\n",
        "### 2.HuggingFacePipeline\n",
        "## huggingfaceendpoint helps hosting the model in remote server and takes care of the architecure but we do have to pay as a service\n",
        "## huggingfacepipeline downloads the model into your local and you would need GPU to host and run the model but it is free of cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mA1DZUPpEND"
      },
      "outputs": [],
      "source": [
        "userdata.get('huggingface_api')\n",
        "from huggingface_hub import login\n",
        "login(token=userdata.get('huggingface_api'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17GcDf6Zabjt"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"google/gemma-3-1b-it\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20000, 'trust_remote_code':True\n",
        "     }\n",
        ")\n",
        "#HuggingFacePipeline will download model weights and host model locally\n",
        "# i did not have enough memory to summary the document,\n",
        "# one can try HuggingFaceEndpoint to use inference providers for the summary but is a paid service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztFpmN0q1I3j"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import load_summarize_chain\n",
        "summarizer=load_summarize_chain(llm=output_llm, chain_type='map_reduce', verbose=True)\n",
        "summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzWKBZT23SKp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "summary=summarizer.invoke(chunks[:9])# cutting chunks only with information\n",
        "vecstore.add_documents(Document(page_content=summary['output_text']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets add a fetaure to look at chat history and answer the questions with less/no context referring to previous chat"
      ],
      "metadata": {
        "id": "EDA-6p6ypL9G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enlm5irapLg9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}