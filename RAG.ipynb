{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sindla97/RAG/blob/dev/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMD_8mwCcquD"
      },
      "source": [
        "# Aproach\n",
        "1. import pdf files to the knowledge base\n",
        "2. Perfrom semantic segmentation using a sentense transfromer type model and convert them to embeddings\n",
        "3. use a rerank model to rank the retrived embeddings\n",
        "4. Use the LLM to generate response based on the query and retrvied emebeddings\n",
        "\n",
        "\n",
        "Try to provide an option to update the knowlege base when new documents are provided,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sMho2se9bUp9"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-google-genai pypdf langchain_experimental langchain-pinecone langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f7qhdF00i8_5"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_path = \"/content/AI_Agent_white_paper_by_google_1737132048.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "loader2=PyPDFLoader(pdf_path, mode='single')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b8qIXqDQ1CYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "bb3af3cb-0ad2-4ae9-95c2-9e24ea73ed95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agents\\n11\\nSeptember 2024\\nd. Action input: The model’s decision on what inputs to provide to the tool (if any)\\ne. Observation: The result of the action / action input sequence\\ni. This thought / action / action input / observation could repeat N-times as needed\\nf. Final answer: The model’s final answer to provide to the original user query\\n4. The ReAct loop concludes and a final answer is provided back to the user\\nFigure 2. Example agent with ReAct reasoning in the orchestration layer\\nAs shown in Figure 2, the model, tools, and agent configuration work together to provide \\na grounded, concise response back to the user based on the user’s original query. While \\nthe model could have guessed at an answer (hallucinated) based on its prior knowledge, \\nit instead used a tool (Flights) to search for real-time external information. This additional \\ninformation was provided to the model, allowing it to make a more informed decision based \\non real factual data and to summarize this information back to the user.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "all_pages=loader.load()\n",
        "# we can use load_and_split() but we have to provide the textsplitter criterion if not it uses Recrusivetextsplitter by default\n",
        "all_pages[10].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading the whole document and invoking the llm for summary and generating embeddings would work for small documents but when the documnet size is large, it has to be chunked and summarized indiviudally and combine to generating embeddings\n"
      ],
      "metadata": {
        "id": "JyJqtQ8ESGaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarize using the whole document"
      ],
      "metadata": {
        "id": "nc8SdLqfTYUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whole_doc=loader2.load() # loading the whole pdf as a single document for answering summary type questions\n",
        "whole_doc[0].page_content"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jcSkYs-lDdYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I6rV34EQm8yq"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "google_api_key=userdata.get('GOOGLE_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ROzrTdWJ0u76"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = SemanticChunker(GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key))\n",
        "chunks = text_splitter.create_documents([''.join([page.page_content for page in all_pages])]) # create document takes in list of dcoumetns and splits it, we have to pass the whole document as string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b0RqkIoB3REl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "a92998e2-ea14-46d2-9c79-f31950066cd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agents\\nAuthors: Julia Wiesinger, Patrick Marlow  \\nand Vladimir VuskovicAgents\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael LanningIntroduction 4\\nWhat is an agent? 5\\n The model 6\\n The tools 7\\n The orchestration layer 7\\n Agents vs. models 8\\n Cognitive architectures: How agents operate  8\\nTools: Our keys to the outside world 12\\n Extensions  13\\n  Sample Extensions  15\\n Functions  18\\n  Use cases 21\\n  Function sample code 24\\n Data stores 27\\n  Implementation and application 28\\n Tools recap 32\\nEnhancing model performance with targeted learning 33\\nAgent quick start with LangChain 35\\nProduction applications with Vertex AI agents 38\\nSummary 40\\nEndnotes 42\\nTable of contentsAgents\\n4\\nSeptember 2024\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "chunks[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "i8I_4jTXEfSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03179fb0-a81a-42b4-a1e9-01c1b50c96ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hjA733Tl9Drl"
      },
      "outputs": [],
      "source": [
        "# importing the llm to summarize the whole document  an retrvial\n",
        "#send the retrived documents to llm and ouptut in a structured format\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "output_llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-lite-preview-06-17\", google_api_key=google_api_key)\n",
        "\n",
        "#lets build a message\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"please provide a summary of this document\"),\n",
        "    (\"human\", f\"please proivde a detailed summary of the document: {whole_doc[0].page_content}\"),\n",
        "]\n",
        "summary=output_llm.invoke(messages)\n",
        "summary # summary of the whole document"
      ],
      "metadata": {
        "id": "p5bhGWs1FE7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "ai_message = summary.content\n",
        "doc = Document(page_content=ai_message) # convert the AI message output to a documemnt\n",
        "\n",
        "print(doc)"
      ],
      "metadata": {
        "id": "6R5Dg20fGwBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MziuiygDp7L4"
      },
      "outputs": [],
      "source": [
        "chunks.append(doc) # append the document tothe other chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "kgW4RNIC4q_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f5779e-6d4e-46ee-8b69-2d88b5842ba5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7aba78705d90>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# lets use a vector store  to store these embeddings\n",
        "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
        "vecstore=PineconeVectorStore(embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key),pinecone_api_key=userdata.get('pinecone_api'), index_name='rag-test')\n",
        "vecstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZqbvWGIqlSA"
      },
      "outputs": [],
      "source": [
        "vecstore.delete(delete_all=True) # delete any existing embeddings in the index\n",
        "vecstore.add_documents(chunks) # create embeddings (based on chunk's text) and add to Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iuklbpxE430i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c4f8f2-306f-4320-a083-3f4fcbeb3778"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='ba7d7da5-529b-4abc-992c-30128de17ec2', metadata={}, page_content=\"6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  \\nhttps://arxiv.org/abs/2305.10601 . 7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  \\nhttps://arxiv.org/abs/2305.08291 .\"),\n",
              " Document(id='2d1eaef7-0b5c-41bc-9e08-68ea15a8a12e', metadata={}, page_content=\"Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  \\nhttps://arxiv.org/pdf/2302.12246.pdf .\"),\n",
              " Document(id='ca04dd70-907a-4f7d-a89f-0dc25546aaf0', metadata={}, page_content='Agents\\nAuthors: Julia Wiesinger, Patrick Marlow  \\nand Vladimir VuskovicAgents\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael LanningIntroduction 4\\nWhat is an agent? 5\\n The model 6\\n The tools 7\\n The orchestration layer 7\\n Agents vs. models 8\\n Cognitive architectures: How agents operate  8\\nTools: Our keys to the outside world 12\\n Extensions  13\\n  Sample Extensions  15\\n Functions  18\\n  Use cases 21\\n  Function sample code 24\\n Data stores 27\\n  Implementation and application 28\\n Tools recap 32\\nEnhancing model performance with targeted learning 33\\nAgent quick start with LangChain 35\\nProduction applications with Vertex AI agents 38\\nSummary 40\\nEndnotes 42\\nTable of contentsAgents\\n4\\nSeptember 2024\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion.'),\n",
              " Document(id='3d302f66-741b-462e-9015-b67627331a27', metadata={}, page_content=\"Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'. Available at:  \\nhttps://arxiv.org/abs/2302.00923 .\")]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "vecstore.search(query='what are the contents of the document Agentic AI', search_type='similarity') # quick check for the retrival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XaKul1SK8rVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ec8d8d-dabb-4b11-af5c-9c30e432dc2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='ba7d7da5-529b-4abc-992c-30128de17ec2', metadata={}, page_content=\"6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  \\nhttps://arxiv.org/abs/2305.10601 . 7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  \\nhttps://arxiv.org/abs/2305.08291 .\"),\n",
              " Document(id='33d19396-5a71-43eb-a337-81a66e201841', metadata={}, page_content='3.'),\n",
              " Document(id='2d1eaef7-0b5c-41bc-9e08-68ea15a8a12e', metadata={}, page_content=\"Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  \\nhttps://arxiv.org/pdf/2302.12246.pdf .\"),\n",
              " Document(id='59d6cc00-e59d-446b-9cd5-9e5ccba721fd', metadata={}, page_content='4.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "vecstore.search(query='what is the summary of the document Agentic AI', search_type='similarity', k=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6QOEGJcbDimx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d404f8b-0ce8-4e1a-a96f-a30e5779d239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Document Summary: Agents**\n",
            "\n",
            "**Authors:** Julia Wiesinger, Patrick Marlow, Vladimir Vuskovic\n",
            "\n",
            "**Contributors:**  Evan Huang, Emily Xue, Olcan Sercinoglu, Sebastian Riedel, Satinder Baveja, Antonio Gulli, Anant Nawalgaria, Grace Mollison, Joey Haymaker, Michael Lanning\n",
            "\n",
            "**Content Overview:**\n",
            "\n",
            "* **Introduction:**  Highlights the human ability for pattern recognition, often enhanced by tools.\n",
            "* **What is an agent?:** Defines the concept of an agent.\n",
            "* **Agent Model, Tools, and Orchestration Layer:** Describes the components of an agent system.\n",
            "* **Agents vs. Models:** Differentiates between agents and models.\n",
            "* **Cognitive Architectures:** Explains how agents function.\n",
            "* **Tools (Extensions, Functions, Data Stores):** Details the external resources agents utilize, including code examples.\n",
            "* **Enhancing Model Performance:** Discusses improving agent capabilities through targeted learning.\n",
            "* **Quick Starts:** Provides guidance on using LangChain and Vertex AI for agent development.\n",
            "* **Production Applications:**  Addresses deploying agents in production environments.\n",
            "* **Summary:**  Concisely summarizes the key concepts.\n",
            "\n",
            "\n",
            "**Follow-up Question:**\n",
            "\n",
            "Given the document's focus on agents leveraging external tools, what specific limitations or challenges are likely encountered when designing and deploying these agents in real-world applications, and how might these be mitigated?\n"
          ]
        }
      ],
      "source": [
        "user_query='what is the summary of the document Agents'\n",
        "retrived_contents=vecstore.search(query=user_query, search_type='similarity', k=4)\n",
        "message=f\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\n",
        "print(output_llm.invoke(message).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Rw1DA-GTEydJ"
      },
      "outputs": [],
      "source": [
        "def chat(user_query):\n",
        "  retrived_contents=vecstore.search(query=user_query, search_type='similarity', k=4)\n",
        "  message=f\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\n",
        "  print(output_llm.invoke(message).content)\n",
        "  new_user_query=input()\n",
        "  if len(new_user_query)>0:\n",
        "    chat(new_user_query)\n",
        "\n",
        "chat('what are the contents of the document Agentic AI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB769BVr1JJv"
      },
      "source": [
        "# lets summarize the whole document and add it to the vector store\n",
        "we can utilize langchain summarize feature but it requires frequent invokes with the llm, if the llm is hosted and accessed through inference endpoints it does cost and trigger alerts for quota and usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWwloSJ4n3QE"
      },
      "source": [
        "## To access models in huggingface we have2 options\n",
        "### 1. using HuggingFaceEndpoint\n",
        "### 2.HuggingFacePipeline\n",
        "## huggingfaceendpoint helps hosting the model in remote server and takes care of the architecure but we do have to pay as a service\n",
        "## huggingfacepipeline downloads the model into your local and you would need GPU to host and run the model but it is free of cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mA1DZUPpEND"
      },
      "outputs": [],
      "source": [
        "userdata.get('huggingface_api')\n",
        "from huggingface_hub import login\n",
        "login(token=userdata.get('huggingface_api'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17GcDf6Zabjt"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"google/gemma-3-1b-it\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20000, 'trust_remote_code':True\n",
        "     }\n",
        ")\n",
        "#HuggingFacePipeline will download model weights and host model locally\n",
        "# i did not have enough memory to summary the document,\n",
        "# one can try HuggingFaceEndpoint to use inference providers for the summary but is a paid service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztFpmN0q1I3j"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import load_summarize_chain\n",
        "summarizer=load_summarize_chain(llm=output_llm, chain_type='map_reduce', verbose=True)\n",
        "summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzWKBZT23SKp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "summary=summarizer.invoke(chunks[:9])# cutting chunks only with information\n",
        "vecstore.add_documents(Document(page_content=summary['output_text']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets add a fetaure to refer to chat history and answer the questions with less/no context"
      ],
      "metadata": {
        "id": "EDA-6p6ypL9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  langchain_core.prompts import BasePromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
        "from  langchain_core.messages import AIMessage,HumanMessage,SystemMessage"
      ],
      "metadata": {
        "id": "enlm5irapLg9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "promt=ChatPromptTemplate.from_messages(#(template='You are an AI assistant named Peanuts please answer user question : {input}')\n",
        "    messages=[\n",
        "        (\"system\", \"You are an AI assistant named {name}.\"),\n",
        "        (\"human\", \"Hi I'm {user}\"),\n",
        "        (\"ai\", \"Hi there, {user}, I'm {name}.\"),\n",
        "    ])"
      ],
      "metadata": {
        "id": "-lcMlZp9uMgn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "promt.invoke({ 'user':'hemanth','name':'Peanuts'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsUQkb7xz0mB",
        "outputId": "062f7037-c1b5-4e9c-fed2-6a649719634e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are an AI assistant named Peanuts.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hi I'm hemanth\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi there, hemanth, I'm Peanuts.\", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_llm.invoke(promt.invoke({'input':'what is your name?', 'user':'hemanth','name':'Peanuts'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKiz--BLzpZI",
        "outputId": "acf6faaf-83f5-40dc-d845-a7cba76512a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='My name is Peanuts.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--a4e72d86-3b7c-45c1-bf4b-3c9f3ec31835-0', usage_metadata={'input_tokens': 32, 'output_tokens': 6, 'total_tokens': 38, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_promt=ChatPromptTemplate.from_messages(messages=[('system','based on the previous chat history can you answer the user question'),\n",
        "                                                     MessagesPlaceholder(variable_name='chat_history'),\n",
        "                                                     ('human','{input}')])"
      ],
      "metadata": {
        "id": "akdcYrAZX9jU"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain=new_promt|output_llm\n",
        "# rag_chain.invoke({'input':'what is your name?', 'user':'hemanth','name':'Peanuts'})"
      ],
      "metadata": {
        "id": "sGQMa8KiSWAC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question=input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7UA_NNChtz2",
        "outputId": "cc7f7524-cbd0-4148-bba5-2d8304d89f8c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "how old am I?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AIoutput=rag_chain.invoke({'input':user_question, 'user':'hemanth','name':'Peanuts','chat_history':promt.invoke({ 'user':'hemanth','name':'Peanuts'}).messages})\n",
        "AIoutput.content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dpiL-Q-IS6L4",
        "outputId": "48626a06-7c42-4ade-b549-37ecf60f5163"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are 25 years old.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "promt.append(HumanMessage(content=user_question))\n",
        "promt.append(AIoutput)"
      ],
      "metadata": {
        "id": "OnDDXr9WhqL1"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "promt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D8MDrQDemys",
        "outputId": "7051e2c1-5a7d-447f-8c32-eac43f1cb3ce"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['name', 'user'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are an AI assistant named {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user'], input_types={}, partial_variables={}, template=\"Hi I'm {user}\"), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name', 'user'], input_types={}, partial_variables={}, template=\"Hi there, {user}, I'm {name}.\"), additional_kwargs={}), HumanMessage(content='what is your name', additional_kwargs={}, response_metadata={}), AIMessage(content='I am Peanuts.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite-preview-06-17', 'safety_ratings': []}, id='run--54f00a9f-b13f-476a-801a-67c9b6d372a2-0', usage_metadata={'input_tokens': 49, 'output_tokens': 5, 'total_tokens': 54, 'input_token_details': {'cache_read': 0}})])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}