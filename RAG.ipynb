{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sindla97/RAG/blob/dev/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMD_8mwCcquD"
      },
      "source": [
        "# Aproach\n",
        "1. import pdf files to the knowledge base\n",
        "2. Perfrom semantic segmentation using a sentense transfromer type model and convert them to embeddings\n",
        "3. use a rerank model to rank the retrived embeddings\n",
        "4. Use the LLM to generate response based on the query and retrvied emebeddings\n",
        "\n",
        "\n",
        "Try to provide an option to update the knowlege base when new documents are provided,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sMho2se9bUp9"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-google-genai pypdf langchain_experimental langchain-pinecone langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f7qhdF00i8_5"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_path = \"/content/AI_Agent_white_paper_by_google_1737132048.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "loader2=PyPDFLoader(pdf_path, mode='single')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b8qIXqDQ1CYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "e9b55082-8962-487d-bb9f-fcdf779863ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agents\\n11\\nSeptember 2024\\nd. Action input: The model’s decision on what inputs to provide to the tool (if any)\\ne. Observation: The result of the action / action input sequence\\ni. This thought / action / action input / observation could repeat N-times as needed\\nf. Final answer: The model’s final answer to provide to the original user query\\n4. The ReAct loop concludes and a final answer is provided back to the user\\nFigure 2. Example agent with ReAct reasoning in the orchestration layer\\nAs shown in Figure 2, the model, tools, and agent configuration work together to provide \\na grounded, concise response back to the user based on the user’s original query. While \\nthe model could have guessed at an answer (hallucinated) based on its prior knowledge, \\nit instead used a tool (Flights) to search for real-time external information. This additional \\ninformation was provided to the model, allowing it to make a more informed decision based \\non real factual data and to summarize this information back to the user.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "all_pages=loader.load()\n",
        "# we can use load_and_split() but we have to provide the textsplitter criterion if not it uses Recrusivetextsplitter by default\n",
        "all_pages[10].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading the whole document and invoking the llm for summary and generating embeddings would work for small documents but when the documnet size is large, it has to be chunked and summarized indiviudally and combine to generating embeddings\n"
      ],
      "metadata": {
        "id": "JyJqtQ8ESGaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarize using the whole document"
      ],
      "metadata": {
        "id": "nc8SdLqfTYUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whole_doc=loader2.load() # loading the whole pdf as a single document for answering summary type questions\n",
        "whole_doc[0].page_content"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jcSkYs-lDdYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I6rV34EQm8yq"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "google_api_key=userdata.get('GOOGLE_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ROzrTdWJ0u76"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = SemanticChunker(GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key))\n",
        "chunks = text_splitter.create_documents([''.join([page.page_content for page in all_pages])]) # create document takes in list of dcoumetns and splits it, we have to pass the whole document as string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b0RqkIoB3REl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "a92998e2-ea14-46d2-9c79-f31950066cd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agents\\nAuthors: Julia Wiesinger, Patrick Marlow  \\nand Vladimir VuskovicAgents\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael LanningIntroduction 4\\nWhat is an agent? 5\\n The model 6\\n The tools 7\\n The orchestration layer 7\\n Agents vs. models 8\\n Cognitive architectures: How agents operate  8\\nTools: Our keys to the outside world 12\\n Extensions  13\\n  Sample Extensions  15\\n Functions  18\\n  Use cases 21\\n  Function sample code 24\\n Data stores 27\\n  Implementation and application 28\\n Tools recap 32\\nEnhancing model performance with targeted learning 33\\nAgent quick start with LangChain 35\\nProduction applications with Vertex AI agents 38\\nSummary 40\\nEndnotes 42\\nTable of contentsAgents\\n4\\nSeptember 2024\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "chunks[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "i8I_4jTXEfSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816ada40-1f6a-400d-c85e-995c509afba8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hjA733Tl9Drl"
      },
      "outputs": [],
      "source": [
        "# importing the llm to summarize the whole document  an retrvial\n",
        "#send the retrived documents to llm and ouptut in a structured format\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "output_llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-lite-preview-06-17\", google_api_key=google_api_key)\n",
        "\n",
        "#lets build a message\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"please provide a summary of this document\"),\n",
        "    (\"human\", f\"please proivde a detailed summary of the document: {whole_doc[0].page_content}\"),\n",
        "]\n",
        "summary=output_llm.invoke(messages)\n",
        "summary # summary of the whole document"
      ],
      "metadata": {
        "id": "p5bhGWs1FE7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "ai_message = summary.content\n",
        "doc = Document(page_content=ai_message) # convert the AI message output to a documemnt\n",
        "\n",
        "print(doc)"
      ],
      "metadata": {
        "id": "6R5Dg20fGwBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MziuiygDp7L4"
      },
      "outputs": [],
      "source": [
        "chunks.append(doc) # append the document tothe other chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "kgW4RNIC4q_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ad1e68-ab21-497b-8b9b-53b5c97ea126"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7d0b0e30e610>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# lets use a vector store  to store these embeddings\n",
        "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
        "vecstore=PineconeVectorStore(embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key),pinecone_api_key=userdata.get('pinecone_api'), index_name='rag-test')\n",
        "vecstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZqbvWGIqlSA"
      },
      "outputs": [],
      "source": [
        "vecstore.delete(delete_all=True) # delete any existing embeddings in the index\n",
        "vecstore.add_documents(chunks) # create embeddings (based on chunk's text) and add to Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuklbpxE430i"
      },
      "outputs": [],
      "source": [
        "vecstore.search(query='what is orchestration layer in the document Agentic AI?', search_type='similarity') # quick check for the retrival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XaKul1SK8rVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ec8d8d-dabb-4b11-af5c-9c30e432dc2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='ba7d7da5-529b-4abc-992c-30128de17ec2', metadata={}, page_content=\"6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  \\nhttps://arxiv.org/abs/2305.10601 . 7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  \\nhttps://arxiv.org/abs/2305.08291 .\"),\n",
              " Document(id='33d19396-5a71-43eb-a337-81a66e201841', metadata={}, page_content='3.'),\n",
              " Document(id='2d1eaef7-0b5c-41bc-9e08-68ea15a8a12e', metadata={}, page_content=\"Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  \\nhttps://arxiv.org/pdf/2302.12246.pdf .\"),\n",
              " Document(id='59d6cc00-e59d-446b-9cd5-9e5ccba721fd', metadata={}, page_content='4.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "vecstore.search(query='what is the summary of the document Agentic AI', search_type='similarity', k=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6QOEGJcbDimx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d404f8b-0ce8-4e1a-a96f-a30e5779d239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Document Summary: Agents**\n",
            "\n",
            "**Authors:** Julia Wiesinger, Patrick Marlow, Vladimir Vuskovic\n",
            "\n",
            "**Contributors:**  Evan Huang, Emily Xue, Olcan Sercinoglu, Sebastian Riedel, Satinder Baveja, Antonio Gulli, Anant Nawalgaria, Grace Mollison, Joey Haymaker, Michael Lanning\n",
            "\n",
            "**Content Overview:**\n",
            "\n",
            "* **Introduction:**  Highlights the human ability for pattern recognition, often enhanced by tools.\n",
            "* **What is an agent?:** Defines the concept of an agent.\n",
            "* **Agent Model, Tools, and Orchestration Layer:** Describes the components of an agent system.\n",
            "* **Agents vs. Models:** Differentiates between agents and models.\n",
            "* **Cognitive Architectures:** Explains how agents function.\n",
            "* **Tools (Extensions, Functions, Data Stores):** Details the external resources agents utilize, including code examples.\n",
            "* **Enhancing Model Performance:** Discusses improving agent capabilities through targeted learning.\n",
            "* **Quick Starts:** Provides guidance on using LangChain and Vertex AI for agent development.\n",
            "* **Production Applications:**  Addresses deploying agents in production environments.\n",
            "* **Summary:**  Concisely summarizes the key concepts.\n",
            "\n",
            "\n",
            "**Follow-up Question:**\n",
            "\n",
            "Given the document's focus on agents leveraging external tools, what specific limitations or challenges are likely encountered when designing and deploying these agents in real-world applications, and how might these be mitigated?\n"
          ]
        }
      ],
      "source": [
        "user_query='what is the summary of the document Agents'\n",
        "retrived_contents=vecstore.search(query=user_query, search_type='similarity', k=4)\n",
        "message=f\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\n",
        "print(output_llm.invoke(message).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Rw1DA-GTEydJ"
      },
      "outputs": [],
      "source": [
        "def chat(user_query):\n",
        "  retrived_contents=vecstore.search(query=user_query, search_type='similarity', k=4)\n",
        "  message=f\"Based on the contents provided {''.join([x.page_content for x in retrived_contents])} answer the user question:{user_query} in a structured format and ask a followup question\"\n",
        "  print(output_llm.invoke(message).content)\n",
        "  new_user_query=input()\n",
        "  if len(new_user_query)>0:\n",
        "    chat(new_user_query)\n",
        "\n",
        "chat('what are the contents of the document Agentic AI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB769BVr1JJv"
      },
      "source": [
        "# lets summarize the whole document and add it to the vector store\n",
        "we can utilize langchain summarize feature but it requires frequent invokes with the llm, if the llm is hosted and accessed through inference endpoints it does cost and trigger alerts for quota and usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWwloSJ4n3QE"
      },
      "source": [
        "## To access models in huggingface we have2 options\n",
        "### 1. using HuggingFaceEndpoint\n",
        "### 2.HuggingFacePipeline\n",
        "## huggingfaceendpoint helps hosting the model in remote server and takes care of the architecure but we do have to pay as a service\n",
        "## huggingfacepipeline downloads the model into your local and you would need GPU to host and run the model but it is free of cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mA1DZUPpEND"
      },
      "outputs": [],
      "source": [
        "userdata.get('huggingface_api')\n",
        "from huggingface_hub import login\n",
        "login(token=userdata.get('huggingface_api'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17GcDf6Zabjt"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"google/gemma-3-1b-it\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20000, 'trust_remote_code':True\n",
        "     }\n",
        ")\n",
        "#HuggingFacePipeline will download model weights and host model locally\n",
        "# i did not have enough memory to summary the document,\n",
        "# one can try HuggingFaceEndpoint to use inference providers for the summary but is a paid service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztFpmN0q1I3j"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import load_summarize_chain\n",
        "summarizer=load_summarize_chain(llm=output_llm, chain_type='map_reduce', verbose=True)\n",
        "summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzWKBZT23SKp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "summary=summarizer.invoke(chunks[:9])# cutting chunks only with information\n",
        "vecstore.add_documents(Document(page_content=summary['output_text']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets add a fetaure to refer to chat history and answer the questions with less/no context"
      ],
      "metadata": {
        "id": "EDA-6p6ypL9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  langchain_core.prompts import BasePromptTemplate, MessagesPlaceholder, ChatPromptTemplate,PromptTemplate\n",
        "from  langchain_core.messages import AIMessage,HumanMessage,SystemMessage\n",
        "from  langchain_core.output_parsers import StrOutputParser,MarkdownListOutputParser"
      ],
      "metadata": {
        "id": "enlm5irapLg9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history=ChatPromptTemplate.from_messages(#(template='You are an AI assistant named Peanuts please answer user question : {input}')\n",
        "    messages=[\n",
        "        (\"system\", \"You are an AI assistant named {name}.\"),\n",
        "        (\"human\", \"Hi I'm {user}\"),\n",
        "        (\"ai\", \"Hi there, {user}, I'm {name}.\"),\n",
        "    ])"
      ],
      "metadata": {
        "id": "-lcMlZp9uMgn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history#.invoke({ 'user':'hemanth','name':'Peanuts'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsUQkb7xz0mB",
        "outputId": "697b07f0-707a-452b-edc4-c143d494e9dc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['name', 'user'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are an AI assistant named {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user'], input_types={}, partial_variables={}, template=\"Hi I'm {user}\"), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name', 'user'], input_types={}, partial_variables={}, template=\"Hi there, {user}, I'm {name}.\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputgen_promt=ChatPromptTemplate.from_messages(messages=[('system','based on the previous chat history and documents retrived  can you answer the user question'),\n",
        "                                                     MessagesPlaceholder(variable_name='chat_history2'),\n",
        "                                                     MessagesPlaceholder(variable_name='documents_retrived'),\n",
        "                                                     ('human','{question}')])\n"
      ],
      "metadata": {
        "id": "akdcYrAZX9jU"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_promt=ChatPromptTemplate.from_messages(messages=[('system','based on the previous provided chat history can you only rewrite the user question but do not answer the user question, if the chat history is not provided please return as it is'),\n",
        "                                                     MessagesPlaceholder(variable_name='chat_history'),\n",
        "                                                     ('human','{input}')])"
      ],
      "metadata": {
        "id": "VMPsO1C8cSlt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_aware_ragchain=retriever_promt|output_llm|StrOutputParser()|vecstore.as_retriever()|(lambda docs: {\"documents_retrived\": [SystemMessage(content=doc.page_content) for doc in docs ], \"question\": input_dict['input'],\"chat_history2\":input_dict['chat_history']} )|outputgen_promt|output_llm|StrOutputParser()\n",
        "# rag_chain.invoke({'input':'what is your name?', 'user':'hemanth','name':'Peanuts'})"
      ],
      "metadata": {
        "id": "sGQMa8KiSWAC"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question=input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7UA_NNChtz2",
        "outputId": "d27b906e-d7e1-4a73-c424-7781cc0a000f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what part it plays in lifecycle of agent?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dict={'input':user_question,'chat_history':chat_history.invoke({ 'user':'hemanth','name':'Peanuts'}).messages}\n",
        "\n",
        "AIoutput=context_aware_ragchain.invoke(input_dict)\n",
        "AIoutput"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "cpMyBEcdgNia",
        "outputId": "a22b7980-68ab-4cc9-8309-9da16f24c3fe"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The orchestration layer plays a crucial role throughout the entire lifecycle of an agent. It acts as the \"brain\" of the agent, dictating how it operates and evolves. Here\\'s a breakdown of its role in the agent\\'s lifecycle:\\n\\n**1. Goal Understanding and Planning:**\\n*   When a user query or a new objective is presented to the agent, the orchestration layer is responsible for interpreting it.\\n*   It breaks down the overall goal into smaller, manageable steps.\\n*   It determines the sequence of actions the agent needs to take, considering the available tools and the agent\\'s current state.\\n\\n**2. Tool Selection and Usage:**\\n*   Based on the current step in the plan, the orchestration layer decides which tool (if any) is most appropriate to use.\\n*   It prepares the necessary input parameters for the selected tool.\\n*   It manages the execution of the tool and receives the \"observation\" or result from it.\\n\\n**3. Reasoning and Decision Making:**\\n*   The orchestration layer uses the information gathered from tools and its own internal reasoning processes to make decisions.\\n*   It evaluates the outcome of an action and decides if further steps are needed or if the current plan needs adjustment.\\n*   This is where reasoning frameworks like ReAct, Chain-of-Thought, or Tree-of-Thoughts come into play, guiding the agent\\'s thought process.\\n\\n**4. State Management and Memory:**\\n*   The orchestration layer maintains the agent\\'s internal state, which includes information about past actions, observations, and the current progress towards the goal.\\n*   This allows the agent to remember context across multiple turns of interaction, enabling it to handle complex, multi-step tasks.\\n\\n**5. Iteration and Refinement:**\\n*   The orchestration layer drives the iterative nature of an agent\\'s operation. It continuously cycles through observing, reasoning, acting, and updating its state.\\n*   If an action doesn\\'t yield the desired result, the orchestration layer can adjust the plan or try a different approach.\\n\\n**6. Goal Achievement and Termination:**\\n*   The orchestration layer monitors the agent\\'s progress towards its goal.\\n*   Once the goal is achieved, it determines the final output to be presented to the user.\\n*   It also manages scenarios where the agent might not be able to achieve the goal, deciding when to stop or report failure.\\n\\n**In essence, the orchestration layer is the continuous loop that powers the agent\\'s entire existence. It\\'s responsible for:**\\n\\n*   **Initiation:** Understanding the starting point or goal.\\n*   **Execution:** Deciding what to do next and how to do it.\\n*   **Adaptation:** Adjusting the plan based on new information or outcomes.\\n*   **Completion:** Recognizing when the goal is met and providing the final result.\\n\\nWithout the orchestration layer, an agent would be a collection of disconnected components (model and tools) unable to perform any meaningful task autonomously.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(HumanMessage(content=user_question))\n",
        "chat_history.append(AIMessage(content=AIoutput))"
      ],
      "metadata": {
        "id": "OnDDXr9WhqL1"
      },
      "execution_count": 63,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}